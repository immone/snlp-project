{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360ccb85-1646-44d7-96f6-c556d1cb1022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/immonej8/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset (assumes a CSV file with 'text' and 'label' columns)\n",
    "df = pd.read_csv(\"train.tsv\", sep='\\t')  # Replace with actual file\n",
    "#print(df.head())\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # Or use nltk.word_tokenize\n",
    "\n",
    "# Build Vocabulary\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['text']), specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Handle out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0675e1c-51a4-4190-8b88-7f8c5d461b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and convert to numerical indices\n",
    "        tokenized = tokenizer(text)\n",
    "        indexed = [vocab[token] for token in tokenized]\n",
    "\n",
    "        # Pad/truncate sequence to max_len\n",
    "        if len(indexed) < self.max_len:\n",
    "            indexed += [vocab[\"<pad>\"]] * (self.max_len - len(indexed))\n",
    "        else:\n",
    "            indexed = indexed[:self.max_len]\n",
    "\n",
    "        return torch.tensor(indexed, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Create dataset\n",
    "dataset = ToxicCommentDataset(df['text'].tolist(), df['label'].tolist(), vocab)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836a8fa1-c282-4a45-aad0-738f795dd1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(FastTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        avg_embeds = embeds.mean(dim=1)  # Mean pooling over sequence\n",
    "        out = self.fc(avg_embeds)  # Fully connected layer\n",
    "        return torch.sigmoid(out).squeeze(1)  # Binary classification\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100  # Can be 50, 100, or 300\n",
    "model = FastTextClassifier(vocab_size, embed_dim, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15082baa-1def-48aa-a9a3-f17eef0701b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./trained_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbc41a5-b0cc-4fe9-94ea-e5cde46b8bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3556\n",
      "Took 268.8 seconds\n",
      "Epoch [2/10], Loss: 0.2678\n",
      "Took 327.5 seconds\n",
      "Epoch [3/10], Loss: 0.2232\n",
      "Took 323.39 seconds\n",
      "Epoch [4/10], Loss: 0.1976\n",
      "Took 323.3 seconds\n",
      "Epoch [5/10], Loss: 0.1803\n",
      "Took 324.11 seconds\n",
      "Epoch [6/10], Loss: 0.1665\n",
      "Took 323.55 seconds\n",
      "Epoch [7/10], Loss: 0.1561\n",
      "Took 323.88 seconds\n",
      "Epoch [8/10], Loss: 0.1466\n",
      "Took 322.7 seconds\n",
      "Epoch [9/10], Loss: 0.1402\n",
      "Took 322.13 seconds\n",
      "Epoch [10/10], Loss: 0.1329\n",
      "Took 321.51 seconds\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(\"Took \" + str(round(time.time()-start, 2)) + \" seconds\")\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10, device=device)\n",
    "torch.save(model.state_dict(), \"./trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c702c8-5d47-4391-9fe6-b9cc60970d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic\n"
     ]
    }
   ],
   "source": [
    "def predict(model, text, vocab, tokenizer, max_len=100, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert to indices\n",
    "    tokenized = tokenizer(text)\n",
    "    indexed = [vocab[token] for token in tokenized]\n",
    "\n",
    "    # Pad/truncate sequence\n",
    "    if len(indexed) < max_len:\n",
    "        indexed += [vocab[\"<pad>\"]] * (max_len - len(indexed))\n",
    "    else:\n",
    "        indexed = indexed[:max_len]\n",
    "\n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(indexed, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get model prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor).item()\n",
    "\n",
    "    return \"Toxic\" if output >= 0.5 else \"Not Toxic\"\n",
    "\n",
    "# Example Prediction\n",
    "sample_text = \"shit\"\n",
    "print(predict(model, sample_text, vocab, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a80ed0-b60b-46be-904d-f362e65fe69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test dataset (assuming a CSV file with 'text' and 'label' columns)\n",
    "df_test = pd.read_csv(\"dev.tsv\", sep='\\t')  # Replace with actual test file\n",
    "# Convert labels to numerical format\n",
    "df_test['label'] = df_test['label'].astype(int)\n",
    "\n",
    "df_ger = df_test[df_test['id'].str.contains('ger')]\n",
    "df_fin = df_test[df_test['id'].str.contains('fin')]\n",
    "df_fin.head()\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = ToxicCommentDataset(df_ger['text'].tolist(), df_ger['label'].tolist(), vocab)\n",
    "\n",
    "# Test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbed1ad7-1ffe-41f5-8753-8a5b30c1bdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7485\n",
      "Precision: 0.4211\n",
      "Recall: 0.0160\n",
      "F1 Score: 0.0308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, test_loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts)  # Forward pass\n",
    "            preds = (outputs >= 0.5).long()  # Convert probabilities to binary labels\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6f877-d997-4148-9422-90e7512e0363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
